{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCM3KT8oWBmyXdVeWhdRnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gopikanta/Natural-Language-Processing---LAB/blob/main/LAB_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXh8VEhSzVSh",
        "outputId": "3d3cebcc-bad7-4f23-ae26-f96600540e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyL3eFS-nIde",
        "outputId": "60e15d73-9283-4919-b143-77dc57c359f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP program no: 1\n",
        "# Read a PDF file and print its first page.\n",
        "\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from PyPDF2 import PdfWriter\n",
        "\n",
        "pdffile = PdfReader(\"samplepage.pdf\")\n",
        "pagecount = len(pdffile.pages)\n",
        "print(f\"\\nPdf Page Length - {pagecount}\")\n",
        "page1 = pdffile.pages[0]\n",
        "text1 = page1.extract_text()\n",
        "print(\"\\nText Contents Of Page 1 >> \\n\")\n",
        "print(text1)\n",
        "print('\\nSaving As \"Result.pdf\" ')\n",
        "pdfresult = page1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsTIf-SxmbmO",
        "outputId": "20f21fce-89a1-4295-fe37-6a06ed487d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pdf Page Length - 2\n",
            "\n",
            "Text Contents Of Page 1 >> \n",
            "\n",
            " A Simple PDF File \n",
            " This is a small demonstration .pdf file - \n",
            " just for use in the Virtual Mechanics tutorials. More text. And more \n",
            " text. And more text. And more text. And more text. \n",
            " And more text. And more text. And more text. And more text. And more \n",
            " text. And more text. Boring, zzzzz. And more text. And more text. And \n",
            " more text. And more text. And more text. And more text. And more text. \n",
            " And more text. And more text. \n",
            " And more text. And more text. And more text. And more text. And more \n",
            " text. And more text. And more text. Even more. Continued on page 2 ...\n",
            "\n",
            "Saving As \"Result.pdf\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsJVtW7HCMxG",
        "outputId": "2f528f91-bf15-4af2-d7a9-92552680818d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrbqgqQCdfR",
        "outputId": "736be7f5-124f-4f43-d2b2-72276d734f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 2\n",
        "# Word to token .\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the sentence\n",
        "file = open(\"Alice.txt\")\n",
        "sentence = file.read()\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5busr09siqC8",
        "outputId": "72054568-69ed-49c2-aefa-5a444cd3d250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'and\", 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', \"'\", 'thought', 'Alice', 'without', 'picure', 'or', 'conversation', \"'\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I2MjVqeFCbc",
        "outputId": "cd5ce0dd-a575-497b-bfbb-f6e9cd583875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no: 3\n",
        "# Remove stopwords from a string.\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "file = open(\"Alice.txt\")\n",
        "sentence = file.read()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "cleaned_text = ' '.join(filtered_words)\n",
        "print(\"Original Text:\")\n",
        "print(sentence)\n",
        "\n",
        "print(\"\\nCleaned Text (without stopwords):\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0ykxt-6_YX2",
        "outputId": "eac5d6c4-1991-4a2f-f862-f0e90add656f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the  book her sister was reading , but it had no pictures or conversations in it, 'and what is the use of a book, ' thought Alice without picure or conversation'\n",
            "\n",
            "Cleaned Text (without stopwords):\n",
            "Alice beginning get tired sitting sister bank , nothing : twice peeped book sister reading , pictures conversations , 'and use book , ' thought Alice without picure conversation '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no: 4\n",
        "\n",
        "# >> Convert text Data into lowercase:\n",
        "\n",
        "input_txt = open(\"Alice.txt\")\n",
        "output_txt = input_txt.read()\n",
        "lowercased_txt = output_txt.lower()\n",
        "print(\"Text in lowercase:\", lowercased_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrPtJANbF88T",
        "outputId": "6eebe351-125e-41b6-a43c-7e176af23c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text in lowercase: alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the  book her sister was reading , but it had no pictures or conversations in it, 'and what is the use of a book, ' thought alice without picure or conversation'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP progran no: 5\n",
        "\n",
        "# >> Remove Punctuations.\n",
        "\n",
        "import string\n",
        "\n",
        "input_txt = open(\"Alice.txt\")\n",
        "output_txt = input_txt.read()\n",
        "\n",
        "\n",
        "# Create a translation table to remove punctuation characters\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Use the translate method to remove punctuations from the sentence\n",
        "cleaned_sentence = output_txt.translate(translator)\n",
        "\n",
        "print(\"Sentence without punctuations:\", cleaned_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Af_xB5HliK",
        "outputId": "e0bcf9ef-5a09-4c49-f641-3b6c01a98a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence without punctuations: Alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the  book her sister was reading  but it had no pictures or conversations in it and what is the use of a book  thought Alice without picure or conversation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 6\n",
        "\n",
        "# >>  Frequency of words.\n",
        "\n",
        "input_txt = open(\"Alice.txt\")\n",
        "output_txt = input_txt.read()\n",
        "words = output_txt.split()\n",
        "words = [word.strip(\".,!?\\\"'(){}[]\") for word in words]\n",
        "\n",
        "word_freq = {}\n",
        "for word in words:\n",
        "    word = word.lower()\n",
        "    if word:\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "print(\"Word Frequencies:\")\n",
        "for word, freq in word_freq.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrNPrIf3JZZD",
        "outputId": "b6e03396-89de-43cd-87d9-6c2444f1988e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequencies:\n",
            "alice: 2\n",
            "was: 2\n",
            "beginning: 1\n",
            "to: 2\n",
            "get: 1\n",
            "very: 1\n",
            "tired: 1\n",
            "of: 3\n",
            "sitting: 1\n",
            "by: 1\n",
            "her: 2\n",
            "sister: 2\n",
            "on: 1\n",
            "the: 3\n",
            "bank: 1\n",
            "and: 2\n",
            "having: 1\n",
            "nothing: 1\n",
            "do:: 1\n",
            "once: 1\n",
            "or: 3\n",
            "twice: 1\n",
            "she: 1\n",
            "had: 2\n",
            "peeped: 1\n",
            "into: 1\n",
            "book: 2\n",
            "reading: 1\n",
            "but: 1\n",
            "it: 2\n",
            "no: 1\n",
            "pictures: 1\n",
            "conversations: 1\n",
            "in: 1\n",
            "what: 1\n",
            "is: 1\n",
            "use: 1\n",
            "a: 1\n",
            "thought: 1\n",
            "without: 1\n",
            "picure: 1\n",
            "conversation: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 7\n",
        "\n",
        "## Extract Entity from a text\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Get input text from the user\n",
        "input_txt = open('Alice.txt')\n",
        "output_txt = input_txt.read()\n",
        "\n",
        "# Process the input text with spaCy\n",
        "doc = nlp(output_txt)\n",
        "\n",
        "# Extract entities and print them\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "if entities:\n",
        "    print(\"Entities:\")\n",
        "    for entity, label in entities:\n",
        "        print(f\"{entity} - {label}\")\n",
        "else:\n",
        "    print(\"No entities found.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivihz_naLEpP",
        "outputId": "88271766-51ff-49d0-ad61-000a09a26a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities:\n",
            "Alice - PERSON\n",
            "Alice - PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 8\n",
        "\n",
        "## Custom Lookup Dictionary and Create a customer Function for text Standardization.\n",
        "\n",
        "lookup_dict = {\n",
        "    \"u\": \"you\",\n",
        "    \"ur\": \"your\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    # Add more custom mappings as needed\n",
        "}\n",
        "\n",
        "\n",
        "def standardize_text(text):\n",
        "    words = text.split()\n",
        "    standardized_text = []\n",
        "\n",
        "    for word in words:\n",
        "        standardized_word = lookup_dict.get(word.lower(), word)\n",
        "        standardized_text.append(standardized_word)\n",
        "\n",
        "    return \" \".join(standardized_text)\n",
        "\n",
        "\n",
        "text = \"u should check this out, btw it's gr8!\"\n",
        "standardized_text = standardize_text(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Standardized Text:\", standardized_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZoD2-sfMyUs",
        "outputId": "2075bc95-7cf3-4d0d-c523-6a5f96bc01f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: u should check this out, btw it's gr8!\n",
            "Standardized Text: you should check this out, by the way it's gr8!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB1Np7YEPsiG",
        "outputId": "22e02af1-6280-4161-eec5-3a37ff14512b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m512.0/622.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=b81794dc8f0f6af15bfc8bca9ec837360d2c83c59bf8c4d1a7f161c626e437c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no: 9\n",
        "\n",
        "# >> Spelling mistake.\n",
        "\n",
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller()\n",
        "\n",
        "error_words = ['samlpe', 'corertc', 'cmoment', 'lodaing']\n",
        "\n",
        "for word in error_words:\n",
        "  corrected_word = spell(word)\n",
        "  print(f\"{word} - > {corrected_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4lGtkRzO3Ub",
        "outputId": "c8d4f8ac-faa3-40a6-eaa0-b7000cd25338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samlpe - > sample\n",
            "corertc - > correct\n",
            "cmoment - > comment\n",
            "lodaing - > loading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJYUSs53l1bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no: 10\n",
        "\n",
        "# >> 10. Perform steaming and levetization on Text.\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "input_txt = open(\"Alice.txt\")\n",
        "output_txt = input_txt.read()\n",
        "\n",
        "\n",
        "# Perform stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in word_tokenize(output_txt)]\n",
        "stemmed_text = ' '.join(stemmed_words)\n",
        "print(\"Stemmed text:\")\n",
        "print(stemmed_text)\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(output_txt)]\n",
        "lemmatized_text = ' '.join(lemmatized_words)\n",
        "print(\"\\nLemmatized text:\")\n",
        "print(lemmatized_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ETm___FTRN7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74df722c-c3cc-4c6e-c294-027c27aae6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed text:\n",
            "alic wa begin to get veri tire of sit by her sister on the bank , and of have noth to do : onc or twice she had peep into the book her sister wa read , but it had no pictur or convers in it , 'and what is the use of a book , ' thought alic without picur or convers '\n",
            "\n",
            "Lemmatized text:\n",
            "Alice wa beginning to get very tired of sitting by her sister on the bank , and of having nothing to do : once or twice she had peeped into the book her sister wa reading , but it had no picture or conversation in it , 'and what is the use of a book , ' thought Alice without picure or conversation '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no: 11\n",
        "\n",
        "# N - Gram\n",
        "\n",
        "input_text = \"This is a sample text for generating n-grams\"\n",
        "n = 3\n",
        "\n",
        "words = input_text.split()\n",
        "ngrams = []\n",
        "\n",
        "for i in range(len(words) - n + 1):\n",
        "    ngrams.append(\" \".join(words[i:i + n]))\n",
        "\n",
        "for ngram in ngrams:\n",
        "    print(ngram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzsReIqsmV1w",
        "outputId": "8b08102c-03f1-437d-f555-e752faaac82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a\n",
            "is a sample\n",
            "a sample text\n",
            "sample text for\n",
            "text for generating\n",
            "for generating n-grams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 12\n",
        "\n",
        "# >> Convert text features using one hot Encoding\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Sample text features as a list of categorical values\n",
        "text_features = ['This is the first picture. ','This is the second picture. ','This is the third picture. ','This is the forth picture. ','Is this  the  first picture ?']\n",
        "\n",
        "# Create a OneHotEncoder object\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Reshape the text_features array into a 2D array (required by the encoder)\n",
        "text_features_reshaped = [[feature] for feature in text_features]\n",
        "\n",
        "# Fit and transform the text_features to obtain the one-hot encoded representation\n",
        "one_hot_encoded_features = encoder.fit_transform(text_features_reshaped).toarray()\n",
        "\n",
        "# Print the one-hot encoded features\n",
        "print(\"One-hot encoded features:\")\n",
        "for feature, encoding in zip(text_features, one_hot_encoded_features):\n",
        "    print(f\"{feature}: {encoding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JByt5jarnP2x",
        "outputId": "f4902d80-fdb9-4fca-9852-a383a2189b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoded features:\n",
            "This is the first picture. : [0. 1. 0. 0. 0.]\n",
            "This is the second picture. : [0. 0. 0. 1. 0.]\n",
            "This is the third picture. : [0. 0. 0. 0. 1.]\n",
            "This is the forth picture. : [0. 0. 1. 0. 0.]\n",
            "Is this  the  first picture ?: [1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 13\n",
        "\n",
        "# >> Convert text Features using a count vectoriser.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text_data = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"And this is the second sentence.\",\n",
        "    \"Finally, the third sentence.\"\n",
        "]\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Convert sparse matrix to dense NumPy array\n",
        "X_dense = X.toarray()\n",
        "\n",
        "# Print the transformed features\n",
        "print(X_dense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16X5MJOQpe9g",
        "outputId": "fac2493b-3998-4685-a3f8-be4b514cdcb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 1 1 0 1]\n",
            " [0 1 0 0 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLF8nC9TxFkQ",
        "outputId": "7bc0eba4-6bcc-4a5d-bb05-259ca017b5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 14\n",
        "\n",
        "# >> POS Tagging.\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "input_sentence = input(\"Enter a sentence: \")\n",
        "words = word_tokenize(input_sentence)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the POS-tagged output\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mQDQBKquM8H",
        "outputId": "beeadfe6-2514-450f-a3df-df168ce1f045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: Kavya is a intelligent girl.\n",
            "[('Kavya', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('intelligent', 'JJ'), ('girl', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Program no : 15\n",
        "\n",
        "# >> Word error rate.\n",
        "\n",
        "import nltk\n",
        "\n",
        "def word_error_rate(reference, hypothesis):\n",
        "  reference = nltk.word_tokenize(reference.lower())\n",
        "  hypothesis = nltk.word_tokenize(hypothesis.lower())\n",
        "  edit_distance = nltk.edit_distance(reference, hypothesis)\n",
        "  word_error_rate_score = edit_distance / len(reference)\n",
        "  return word_error_rate_score\n",
        "reference =  \"She sells seashells by the seashore\"\n",
        "hypothesis =  \"He sells seashells by the sea\"\n",
        "word_error_rate_score = word_error_rate(reference, hypothesis)\n",
        "print(\"Word error rate : \", word_error_rate_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypACyrTXxj4e",
        "outputId": "a7ebff47-81dd-424b-d06d-988468c3d48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word error rate :  0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gq-6F1DM78O9"
      }
    }
  ]
}